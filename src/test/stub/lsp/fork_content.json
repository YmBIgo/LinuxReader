[
    {
        "line": 240,
        "character": 11,
        "functionName": "free_vm_stack_cache",
        "firstLine": "static int free_vm_stack_cache(unsigned int cpu)",
        "content": "static int free_vm_stack_cache(unsigned int cpu)\n{\n\tstruct vm_struct **cached_vm_stacks = per_cpu_ptr(cached_stacks, cpu);\n\tint i;\n\n\tfor (i = 0; i < NR_CACHED_STACKS; i++) {\n\t\tstruct vm_struct *vm_stack = cached_vm_stacks[i];\n\n\t\tif (!vm_stack)\n\t\t\tcontinue;\n\n\t\tvfree(vm_stack->addr);\n\t\tcached_vm_stacks[i] = NULL;\n\t}\n\n\treturn 0;\n}"
    },
    {
        "line": 232,
        "character": 12,
        "functionName": "thread_stack_delayed_free",
        "firstLine": "static void thread_stack_delayed_free(struct task_struct *tsk)",
        "content" : "static void thread_stack_delayed_free(struct task_struct *tsk)\n{\n\tstruct vm_stack *vm_stack = tsk->stack;\n\n\tvm_stack->stack_vm_area = tsk->stack_vm_area;\n\tcall_rcu(&vm_stack->rcu, thread_stack_free_rcu);\n}"
    },
    {
        "line": 279,
        "character": 11,
        "functionName": "alloc_thread_stack_node",
        "firstLine": "static int alloc_thread_stack_node(struct task_struct *tsk, int node)",
        "content": "static int alloc_thread_stack_node(struct task_struct *tsk, int node)\n{\n\tstruct vm_struct *vm;\n\tvoid *stack;\n\tint i;\n\n\tfor (i = 0; i < NR_CACHED_STACKS; i++) {\n\t\tstruct vm_struct *s;\n\n\t\ts = this_cpu_xchg(cached_stacks[i], NULL);\n\n\t\tif (!s)\n\t\t\tcontinue;\n\n\t\t/* Reset stack metadata. */\n\t\tkasan_unpoison_range(s->addr, THREAD_SIZE);\n\n\t\tstack = kasan_reset_tag(s->addr);\n\n\t\t/* Clear stale pointers from reused stack. */\n\t\tmemset(stack, 0, THREAD_SIZE);\n\n\t\tif (memcg_charge_kernel_stack(s)) {\n\t\t\tvfree(s->addr);\n\t\t\treturn -ENOMEM;\n\t\t}\n\n\t\ttsk->stack_vm_area = s;\n\t\ttsk->stack = stack;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Allocated stacks are cached and later reused by new threads,\n\t * so memcg accounting is performed manually on assigning/releasing\n\t * stacks to tasks. Drop __GFP_ACCOUNT.\n\t */\n\tstack = __vmalloc_node(THREAD_SIZE, THREAD_ALIGN,\n\t\t\t\t     THREADINFO_GFP & ~__GFP_ACCOUNT,\n\t\t\t\t     node, __builtin_return_address(0));\n\tif (!stack)\n\t\treturn -ENOMEM;\n\n\tvm = find_vm_area(stack);\n\tif (memcg_charge_kernel_stack(vm)) {\n\t\tvfree(stack);\n\t\treturn -ENOMEM;\n\t}\n\t/*\n\t * We can't call find_vm_area() in interrupt context, and\n\t * free_thread_stack() can be called in interrupt context,\n\t * so cache the vm_struct.\n\t */\n\ttsk->stack_vm_area = vm;\n\tstack = kasan_reset_tag(stack);\n\ttsk->stack = stack;\n\treturn 0;\n}"
    },
    {
        "line": 1912,
        "character": 37,
        "functionName": "copy_process",
        "firstLine": "__latent_entropy struct task_struct *copy_process(",
        "content": "__latent_entropy struct task_struct *copy_process(\n\t\t\t\t\tstruct pid *pid,\n\t\t\t\t\tint trace,\n\t\t\t\t\tint node,\n\t\t\t\t\tstruct kernel_clone_args *args)\n{\n\tint pidfd = -1, retval;\n\tstruct task_struct *p;\n\tstruct multiprocess_signals delayed;\n\tstruct file *pidfile = NULL;\n\tconst u64 clone_flags = args->flags;\n\tstruct nsproxy *nsp = current->nsproxy;\n\n\t/*\n\t * Don't allow sharing the root directory with processes in a different\n\t * namespace\n\t */\n\tif ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\tif ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Thread groups must share signals as well, and detached threads\n\t * can only be started up within the thread group.\n\t */\n\tif ((clone_flags & CLONE_THREAD) && !(clone_flags & CLONE_SIGHAND))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Shared signal handlers imply shared VM. By way of the above,\n\t * thread groups also imply shared VM. Blocking this case allows\n\t * for various simplifications in other code.\n\t */\n\tif ((clone_flags & CLONE_SIGHAND) && !(clone_flags & CLONE_VM))\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * Siblings of global init remain as zombies on exit since they are\n\t * not reaped by their parent (swapper). To solve this and to avoid\n\t * multi-rooted process trees, prevent global and container-inits\n\t * from creating siblings.\n\t */\n\tif ((clone_flags & CLONE_PARENT) &&\n\t\t\t\tcurrent->signal->flags & SIGNAL_UNKILLABLE)\n\t\treturn ERR_PTR(-EINVAL);\n\n\t/*\n\t * If the new process will be in a different pid or user namespace\n\t * do not allow it to share a thread group with the forking task.\n\t */\n\tif (clone_flags & CLONE_THREAD) {\n\t\tif ((clone_flags & (CLONE_NEWUSER | CLONE_NEWPID)) ||\n\t\t    (task_active_pid_ns(current) != nsp->pid_ns_for_children))\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\tif (clone_flags & CLONE_PIDFD) {\n\t\t/*\n\t\t * - CLONE_DETACHED is blocked so that we can potentially\n\t\t *   reuse it later for CLONE_PIDFD.\n\t\t */\n\t\tif (clone_flags & CLONE_DETACHED)\n\t\t\treturn ERR_PTR(-EINVAL);\n\t}\n\n\t/*\n\t * Force any signals received before this point to be delivered\n\t * before the fork happens.  Collect up signals sent to multiple\n\t * processes that happen during the fork and delay them so that\n\t * they appear to happen after the fork.\n\t */\n\tsigemptyset(&delayed.signal);\n\tINIT_HLIST_NODE(&delayed.node);\n\n\tspin_lock_irq(&current->sighand->siglock);\n\tif (!(clone_flags & CLONE_THREAD))\n\t\thlist_add_head(&delayed.node, &current->signal->multiprocess);\n\trecalc_sigpending();\n\tspin_unlock_irq(&current->sighand->siglock);\n\tretval = -ERESTARTNOINTR;\n\tif (task_sigpending(current))\n\t\tgoto fork_out;\n\n\tretval = -ENOMEM;\n\tp = dup_task_struct(current, node);\n\tif (!p)\n\t\tgoto fork_out;\n\tp->flags &= ~PF_KTHREAD;\n\tif (args->kthread)\n\t\tp->flags |= PF_KTHREAD;\n\tif (args->user_worker) {\n\t\t/*\n\t\t * Mark us a user worker, and block any signal that isn't\n\t\t * fatal or STOP\n\t\t */\n\t\tp->flags |= PF_USER_WORKER;\n\t\tsiginitsetinv(&p->blocked, sigmask(SIGKILL)|sigmask(SIGSTOP));\n\t}\n\tif (args->io_thread)\n\t\tp->flags |= PF_IO_WORKER;\n\n\tif (args->name)\n\t\tstrscpy_pad(p->comm, args->name, sizeof(p->comm));\n\n\tp->set_child_tid = (clone_flags & CLONE_CHILD_SETTID) ? args->child_tid : NULL;\n\t/*\n\t * Clear TID on mm_release()?\n\t */\n\tp->clear_child_tid = (clone_flags & CLONE_CHILD_CLEARTID) ? args->child_tid : NULL;\n\n\tftrace_graph_init_task(p);\n\n\trt_mutex_init_task(p);\n\n\tlockdep_assert_irqs_enabled();\n#ifdef CONFIG_PROVE_LOCKING\n\tDEBUG_LOCKS_WARN_ON(!p->softirqs_enabled);\n#endif\n\tretval = copy_creds(p, clone_flags);\n\tif (retval < 0)\n\t\tgoto bad_fork_free;\n\n\tretval = -EAGAIN;\n\tif (is_rlimit_overlimit(task_ucounts(p), UCOUNT_RLIMIT_NPROC, rlimit(RLIMIT_NPROC))) {\n\t\tif (p->real_cred->user != INIT_USER &&\n\t\t    !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN))\n\t\t\tgoto bad_fork_cleanup_count;\n\t}\n\tcurrent->flags &= ~PF_NPROC_EXCEEDED;\n\n\t/*\n\t * If multiple threads are within copy_process(), then this check\n\t * triggers too late. This doesn't hurt, the check is only there\n\t * to stop root fork bombs.\n\t */\n\tretval = -EAGAIN;\n\tif (data_race(nr_threads >= max_threads))\n\t\tgoto bad_fork_cleanup_count;\n\n\tdelayacct_tsk_init(p);\t/* Must remain after dup_task_struct() */\n\tp->flags &= ~(PF_SUPERPRIV | PF_WQ_WORKER | PF_IDLE | PF_NO_SETAFFINITY);\n\tp->flags |= PF_FORKNOEXEC;\n\tINIT_LIST_HEAD(&p->children);\n\tINIT_LIST_HEAD(&p->sibling);\n\trcu_copy_process(p);\n\tp->vfork_done = NULL;\n\tspin_lock_init(&p->alloc_lock);\n\n\tinit_sigpending(&p->pending);\n\n\tp->utime = p->stime = p->gtime = 0;\n#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME\n\tp->utimescaled = p->stimescaled = 0;\n#endif\n\tprev_cputime_init(&p->prev_cputime);\n\n#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\n\tseqcount_init(&p->vtime.seqcount);\n\tp->vtime.starttime = 0;\n\tp->vtime.state = VTIME_INACTIVE;\n#endif\n\n#ifdef CONFIG_IO_URING\n\tp->io_uring = NULL;\n#endif\n\n\tp->default_timer_slack_ns = current->timer_slack_ns;\n\n#ifdef CONFIG_PSI\n\tp->psi_flags = 0;\n#endif\n\n\ttask_io_accounting_init(&p->ioac);\n\tacct_clear_integrals(p);\n\n\tposix_cputimers_init(&p->posix_cputimers);\n\ttick_dep_init_task(p);\n\n\tp->io_context = NULL;\n\taudit_set_context(p, NULL);\n\tcgroup_fork(p);\n\tif (args->kthread) {\n\t\tif (!set_kthread_struct(p))\n\t\t\tgoto bad_fork_cleanup_delayacct;\n\t}\n#ifdef CONFIG_NUMA\n\tp->mempolicy = mpol_dup(p->mempolicy);\n\tif (IS_ERR(p->mempolicy)) {\n\t\tretval = PTR_ERR(p->mempolicy);\n\t\tp->mempolicy = NULL;\n\t\tgoto bad_fork_cleanup_delayacct;\n\t}\n#endif\n#ifdef CONFIG_CPUSETS\n\tp->cpuset_mem_spread_rotor = NUMA_NO_NODE;\n\tseqcount_spinlock_init(&p->mems_allowed_seq, &p->alloc_lock);\n#endif\n#ifdef CONFIG_TRACE_IRQFLAGS\n\tmemset(&p->irqtrace, 0, sizeof(p->irqtrace));\n\tp->irqtrace.hardirq_disable_ip\t= _THIS_IP_;\n\tp->irqtrace.softirq_enable_ip\t= _THIS_IP_;\n\tp->softirqs_enabled\t\t= 1;\n\tp->softirq_context\t\t= 0;\n#endif\n\n\tp->pagefault_disabled = 0;\n\n#ifdef CONFIG_LOCKDEP\n\tlockdep_init_task(p);\n#endif\n\n#ifdef CONFIG_DEBUG_MUTEXES\n\tp->blocked_on = NULL; /* not blocked yet */\n#endif\n#ifdef CONFIG_BCACHE\n\tp->sequential_io\t= 0;\n\tp->sequential_io_avg\t= 0;\n#endif\n#ifdef CONFIG_BPF_SYSCALL\n\tRCU_INIT_POINTER(p->bpf_storage, NULL);\n\tp->bpf_ctx = NULL;\n#endif\n\n\t/* Perform scheduler related setup. Assign this task to a CPU. */\n\tretval = sched_fork(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_policy;\n\n\tretval = perf_event_init_task(p, clone_flags);\n\tif (retval)\n\t\tgoto bad_fork_sched_cancel_fork;\n\tretval = audit_alloc(p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_perf;\n\t/* copy all the process information */\n\tshm_init_task(p);\n\tretval = security_task_alloc(p, clone_flags);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_audit;\n\tretval = copy_semundo(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_security;\n\tretval = copy_files(clone_flags, p, args->no_files);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_semundo;\n\tretval = copy_fs(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_files;\n\tretval = copy_sighand(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_fs;\n\tretval = copy_signal(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_sighand;\n\tretval = copy_mm(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_signal;\n\tretval = copy_namespaces(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_mm;\n\tretval = copy_io(clone_flags, p);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_namespaces;\n\tretval = copy_thread(p, args);\n\tif (retval)\n\t\tgoto bad_fork_cleanup_io;\n\n\tstackleak_task_init(p);\n\n\tif (pid != &init_struct_pid) {\n\t\tpid = alloc_pid(p->nsproxy->pid_ns_for_children, args->set_tid,\n\t\t\t\targs->set_tid_size);\n\t\tif (IS_ERR(pid)) {\n\t\t\tretval = PTR_ERR(pid);\n\t\t\tgoto bad_fork_cleanup_thread;\n\t\t}\n\t}\n\n\t/*\n\t * This has to happen after we've potentially unshared the file\n\t * descriptor table (so that the pidfd doesn't leak into the child\n\t * if the fd table isn't shared).\n\t */\n\tif (clone_flags & CLONE_PIDFD) {\n\t\tint flags = (clone_flags & CLONE_THREAD) ? PIDFD_THREAD : 0;\n\n\t\t/*\n\t\t * Note that no task has been attached to @pid yet indicate\n\t\t * that via CLONE_PIDFD.\n\t\t */\n\t\tretval = pidfd_prepare(pid, flags | PIDFD_STALE, &pidfile);\n\t\tif (retval < 0)\n\t\t\tgoto bad_fork_free_pid;\n\t\tpidfd = retval;\n\n\t\tretval = put_user(pidfd, args->pidfd);\n\t\tif (retval)\n\t\t\tgoto bad_fork_put_pidfd;\n\t}\n\n#ifdef CONFIG_BLOCK\n\tp->plug = NULL;\n#endif\n\tfutex_init_task(p);\n\n\t/*\n\t * sigaltstack should be cleared when sharing the same VM\n\t */\n\tif ((clone_flags & (CLONE_VM|CLONE_VFORK)) == CLONE_VM)\n\t\tsas_ss_reset(p);\n\n\t/*\n\t * Syscall tracing and stepping should be turned off in the\n\t * child regardless of CLONE_PTRACE.\n\t */\n\tuser_disable_single_step(p);\n\tclear_task_syscall_work(p, SYSCALL_TRACE);\n#if defined(CONFIG_GENERIC_ENTRY) || defined(TIF_SYSCALL_EMU)\n\tclear_task_syscall_work(p, SYSCALL_EMU);\n#endif\n\tclear_tsk_latency_tracing(p);\n\n\t/* ok, now we should be set up.. */\n\tp->pid = pid_nr(pid);\n\tif (clone_flags & CLONE_THREAD) {\n\t\tp->group_leader = current->group_leader;\n\t\tp->tgid = current->tgid;\n\t} else {\n\t\tp->group_leader = p;\n\t\tp->tgid = p->pid;\n\t}\n\n\tp->nr_dirtied = 0;\n\tp->nr_dirtied_pause = 128 >> (PAGE_SHIFT - 10);\n\tp->dirty_paused_when = 0;\n\n\tp->pdeath_signal = 0;\n\tp->task_works = NULL;\n\tclear_posix_cputimers_work(p);\n\n#ifdef CONFIG_KRETPROBES\n\tp->kretprobe_instances.first = NULL;\n#endif\n#ifdef CONFIG_RETHOOK\n\tp->rethooks.first = NULL;\n#endif\n\n\t/*\n\t * Ensure that the cgroup subsystem policies allow the new process to be\n\t * forked. It should be noted that the new process's css_set can be changed\n\t * between here and cgroup_post_fork() if an organisation operation is in\n\t * progress.\n\t */\n\tretval = cgroup_can_fork(p, args);\n\tif (retval)\n\t\tgoto bad_fork_put_pidfd;\n\n\t/*\n\t * Now that the cgroups are pinned, re-clone the parent cgroup and put\n\t * the new task on the correct runqueue. All this *before* the task\n\t * becomes visible.\n\t *\n\t * This isn't part of ->can_fork() because while the re-cloning is\n\t * cgroup specific, it unconditionally needs to place the task on a\n\t * runqueue.\n\t */\n\tretval = sched_cgroup_fork(p, args);\n\tif (retval)\n\t\tgoto bad_fork_cancel_cgroup;\n\n\t/*\n\t * Allocate a default futex hash for the user process once the first\n\t * thread spawns.\n\t */\n\tif (need_futex_hash_allocate_default(clone_flags)) {\n\t\tretval = futex_hash_allocate_default();\n\t\tif (retval)\n\t\t\tgoto bad_fork_core_free;\n\t\t/*\n\t\t * If we fail beyond this point we don't free the allocated\n\t\t * futex hash map. We assume that another thread will be created\n\t\t * and makes use of it. The hash map will be freed once the main\n\t\t * thread terminates.\n\t\t */\n\t}\n\t/*\n\t * From this point on we must avoid any synchronous user-space\n\t * communication until we take the tasklist-lock. In particular, we do\n\t * not want user-space to be able to predict the process start-time by\n\t * stalling fork(2) after we recorded the start_time but before it is\n\t * visible to the system.\n\t */\n\n\tp->start_time = ktime_get_ns();\n\tp->start_boottime = ktime_get_boottime_ns();\n\n\t/*\n\t * Make it visible to the rest of the system, but dont wake it up yet.\n\t * Need tasklist lock for parent etc handling!\n\t */\n\twrite_lock_irq(&tasklist_lock);\n\n\t/* CLONE_PARENT re-uses the old parent */\n\tif (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {\n\t\tp->real_parent = current->real_parent;\n\t\tp->parent_exec_id = current->parent_exec_id;\n\t\tif (clone_flags & CLONE_THREAD)\n\t\t\tp->exit_signal = -1;\n\t\telse\n\t\t\tp->exit_signal = current->group_leader->exit_signal;\n\t} else {\n\t\tp->real_parent = current;\n\t\tp->parent_exec_id = current->self_exec_id;\n\t\tp->exit_signal = args->exit_signal;\n\t}\n\n\tklp_copy_process(p);\n\n\tsched_core_fork(p);\n\n\tspin_lock(&current->sighand->siglock);\n\n\trv_task_fork(p);\n\n\trseq_fork(p, clone_flags);\n\n\t/* Don't start children in a dying pid namespace */\n\tif (unlikely(!(ns_of_pid(pid)->pid_allocated & PIDNS_ADDING))) {\n\t\tretval = -ENOMEM;\n\t\tgoto bad_fork_core_free;\n\t}\n\n\t/* Let kill terminate clone/fork in the middle */\n\tif (fatal_signal_pending(current)) {\n\t\tretval = -EINTR;\n\t\tgoto bad_fork_core_free;\n\t}\n\n\t/* No more failure paths after this point. */\n\n\t/*\n\t * Copy seccomp details explicitly here, in case they were changed\n\t * before holding sighand lock.\n\t */\n\tcopy_seccomp(p);\n\n\tinit_task_pid_links(p);\n\tif (likely(p->pid)) {\n\t\tptrace_init_task(p, (clone_flags & CLONE_PTRACE) || trace);\n\n\t\tinit_task_pid(p, PIDTYPE_PID, pid);\n\t\tif (thread_group_leader(p)) {\n\t\t\tinit_task_pid(p, PIDTYPE_TGID, pid);\n\t\t\tinit_task_pid(p, PIDTYPE_PGID, task_pgrp(current));\n\t\t\tinit_task_pid(p, PIDTYPE_SID, task_session(current));\n\n\t\t\tif (is_child_reaper(pid)) {\n\t\t\t\tns_of_pid(pid)->child_reaper = p;\n\t\t\t\tp->signal->flags |= SIGNAL_UNKILLABLE;\n\t\t\t}\n\t\t\tp->signal->shared_pending.signal = delayed.signal;\n\t\t\tp->signal->tty = tty_kref_get(current->signal->tty);\n\t\t\t/*\n\t\t\t * Inherit has_child_subreaper flag under the same\n\t\t\t * tasklist_lock with adding child to the process tree\n\t\t\t * for propagate_has_child_subreaper optimization.\n\t\t\t */\n\t\t\tp->signal->has_child_subreaper = p->real_parent->signal->has_child_subreaper ||\n\t\t\t\t\t\t\t p->real_parent->signal->is_child_subreaper;\n\t\t\tlist_add_tail(&p->sibling, &p->real_parent->children);\n\t\t\tlist_add_tail_rcu(&p->tasks, &init_task.tasks);\n\t\t\tattach_pid(p, PIDTYPE_TGID);\n\t\t\tattach_pid(p, PIDTYPE_PGID);\n\t\t\tattach_pid(p, PIDTYPE_SID);\n\t\t\t__this_cpu_inc(process_counts);\n\t\t} else {\n\t\t\tcurrent->signal->nr_threads++;\n\t\t\tcurrent->signal->quick_threads++;\n\t\t\tatomic_inc(&current->signal->live);\n\t\t\trefcount_inc(&current->signal->sigcnt);\n\t\t\ttask_join_group_stop(p);\n\t\t\tlist_add_tail_rcu(&p->thread_node,\n\t\t\t\t\t  &p->signal->thread_head);\n\t\t}\n\t\tattach_pid(p, PIDTYPE_PID);\n\t\tnr_threads++;\n\t}\n\ttotal_forks++;\n\thlist_del_init(&delayed.node);\n\tspin_unlock(&current->sighand->siglock);\n\tsyscall_tracepoint_update(p);\n\twrite_unlock_irq(&tasklist_lock);\n\n\tif (pidfile)\n\t\tfd_install(pidfd, pidfile);\n\n\tproc_fork_connector(p);\n\tsched_post_fork(p);\n\tcgroup_post_fork(p, args);\n\tperf_event_fork(p);\n\n\ttrace_task_newtask(p, clone_flags);\n\tuprobe_copy_process(p, clone_flags);\n\tuser_events_fork(p, clone_flags);\n\n\tcopy_oom_score_adj(clone_flags, p);\n\n\treturn p;\n\nbad_fork_core_free:\n\tsched_core_free(p);\n\tspin_unlock(&current->sighand->siglock);\n\twrite_unlock_irq(&tasklist_lock);\nbad_fork_cancel_cgroup:\n\tcgroup_cancel_fork(p, args);\nbad_fork_put_pidfd:\n\tif (clone_flags & CLONE_PIDFD) {\n\t\tfput(pidfile);\n\t\tput_unused_fd(pidfd);\n\t}\nbad_fork_free_pid:\n\tif (pid != &init_struct_pid)\n\t\tfree_pid(pid);\nbad_fork_cleanup_thread:\n\texit_thread(p);\nbad_fork_cleanup_io:\n\tif (p->io_context)\n\t\texit_io_context(p);\nbad_fork_cleanup_namespaces:\n\texit_task_namespaces(p);\nbad_fork_cleanup_mm:\n\tif (p->mm) {\n\t\tmm_clear_owner(p->mm, p);\n\t\tmmput(p->mm);\n\t}\nbad_fork_cleanup_signal:\n\tif (!(clone_flags & CLONE_THREAD))\n\t\tfree_signal_struct(p->signal);\nbad_fork_cleanup_sighand:\n\t__cleanup_sighand(p->sighand);\nbad_fork_cleanup_fs:\n\texit_fs(p); /* blocking */\nbad_fork_cleanup_files:\n\texit_files(p); /* blocking */\nbad_fork_cleanup_semundo:\n\texit_sem(p);\nbad_fork_cleanup_security:\n\tsecurity_task_free(p);\nbad_fork_cleanup_audit:\n\taudit_free(p);\nbad_fork_cleanup_perf:\n\tperf_event_free_task(p);\nbad_fork_sched_cancel_fork:\n\tsched_cancel_fork(p);\nbad_fork_cleanup_policy:\n\tlockdep_free_task(p);\n#ifdef CONFIG_NUMA\n\tmpol_put(p->mempolicy);\n#endif\nbad_fork_cleanup_delayacct:\n\tdelayacct_tsk_free(p);\nbad_fork_cleanup_count:\n\tdec_rlimit_ucounts(task_ucounts(p), UCOUNT_RLIMIT_NPROC, 1);\n\texit_creds(p);\nbad_fork_free:\n\tWRITE_ONCE(p->__state, TASK_DEAD);\n\texit_task_stack_account(p);\n\tput_task_stack(p);\n\tdelayed_free_task(p);\nfork_out:\n\tspin_lock_irq(&current->sighand->siglock);\n\thlist_del_init(&delayed.node);\n\tspin_unlock_irq(&current->sighand->siglock);\n\treturn ERR_PTR(retval);\n}"
    },
    {
        "line": 2557,
        "character": 6,
        "functionName": "kernel_clone",
        "firstLine": "pid_t kernel_clone(struct kernel_clone_args *args)",
        "content": "pid_t kernel_clone(struct kernel_clone_args *args)\n{\n\tu64 clone_flags = args->flags;\n\tstruct completion vfork;\n\tstruct pid *pid;\n\tstruct task_struct *p;\n\tint trace = 0;\n\tpid_t nr;\n\n\t/*\n\t * For legacy clone() calls, CLONE_PIDFD uses the parent_tid argument\n\t * to return the pidfd. Hence, CLONE_PIDFD and CLONE_PARENT_SETTID are\n\t * mutually exclusive. With clone3() CLONE_PIDFD has grown a separate\n\t * field in struct clone_args and it still doesn't make sense to have\n\t * them both point at the same memory location. Performing this check\n\t * here has the advantage that we don't need to have a separate helper\n\t * to check for legacy clone().\n\t */\n\tif ((clone_flags & CLONE_PIDFD) &&\n\t    (clone_flags & CLONE_PARENT_SETTID) &&\n\t    (args->pidfd == args->parent_tid))\n\t\treturn -EINVAL;\n\n\t/*\n\t * Determine whether and which event to report to ptracer.  When\n\t * called from kernel_thread or CLONE_UNTRACED is explicitly\n\t * requested, no event is reported; otherwise, report if the event\n\t * for the type of forking is enabled.\n\t */\n\tif (!(clone_flags & CLONE_UNTRACED)) {\n\t\tif (clone_flags & CLONE_VFORK)\n\t\t\ttrace = PTRACE_EVENT_VFORK;\n\t\telse if (args->exit_signal != SIGCHLD)\n\t\t\ttrace = PTRACE_EVENT_CLONE;\n\t\telse\n\t\t\ttrace = PTRACE_EVENT_FORK;\n\n\t\tif (likely(!ptrace_event_enabled(current, trace)))\n\t\t\ttrace = 0;\n\t}\n\n\tp = copy_process(NULL, trace, NUMA_NO_NODE, args);\n\tadd_latent_entropy();\n\n\tif (IS_ERR(p))\n\t\treturn PTR_ERR(p);\n\n\t/*\n\t * Do this prior waking up the new thread - the thread pointer\n\t * might get invalid after that point, if the thread exits quickly.\n\t */\n\ttrace_sched_process_fork(current, p);\n\n\tpid = get_task_pid(p, PIDTYPE_PID);\n\tnr = pid_vnr(pid);\n\n\tif (clone_flags & CLONE_PARENT_SETTID)\n\t\tput_user(nr, args->parent_tid);\n\n\tif (clone_flags & CLONE_VFORK) {\n\t\tp->vfork_done = &vfork;\n\t\tinit_completion(&vfork);\n\t\tget_task_struct(p);\n\t}\n\n\tif (IS_ENABLED(CONFIG_LRU_GEN_WALKS_MMU) && !(clone_flags & CLONE_VM)) {\n\t\t/* lock the task to synchronize with memcg migration */\n\t\ttask_lock(p);\n\t\tlru_gen_add_mm(p->mm);\n\t\ttask_unlock(p);\n\t}\n\n\twake_up_new_task(p);\n\n\t/* forking complete and child started to run, tell ptracer */\n\tif (unlikely(trace))\n\t\tptrace_event_pid(trace, pid);\n\n\tif (clone_flags & CLONE_VFORK) {\n\t\tif (!wait_for_vfork_done(p, &vfork))\n\t\t\tptrace_event_pid(PTRACE_EVENT_VFORK_DONE, pid);\n\t}\n\n\tput_pid(pid);\n\treturn nr;\n}"
    }, 
    {
        "line": 1024,
        "character": 25,
        "functionName": "mm_init",
        "firstLine": "static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,",
        "content": "static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,\n\tstruct user_namespace *user_ns)\n{\n\tmt_init_flags(&mm->mm_mt, MM_MT_FLAGS);\n\tmt_set_external_lock(&mm->mm_mt, &mm->mmap_lock);\n\tatomic_set(&mm->mm_users, 1);\n\tatomic_set(&mm->mm_count, 1);\n\tseqcount_init(&mm->write_protect_seq);\n\tmmap_init_lock(mm);\n\tINIT_LIST_HEAD(&mm->mmlist);\n\tmm_pgtables_bytes_init(mm);\n\tmm->map_count = 0;\n\tmm->locked_vm = 0;\n\tatomic64_set(&mm->pinned_vm, 0);\n\tmemset(&mm->rss_stat, 0, sizeof(mm->rss_stat));\n\tspin_lock_init(&mm->page_table_lock);\n\tspin_lock_init(&mm->arg_lock);\n\tmm_init_cpumask(mm);\n\tmm_init_aio(mm);\n\tmm_init_owner(mm, p);\n\tmm_pasid_init(mm);\n\tRCU_INIT_POINTER(mm->exe_file, NULL);\n\tmmu_notifier_subscriptions_init(mm);\n\tinit_tlb_flush_pending(mm);\n\tfutex_mm_init(mm);\n#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !defined(CONFIG_SPLIT_PMD_PTLOCKS)\n\tmm->pmd_huge_pte = NULL;\n#endif\n\tmm_init_uprobes_state(mm);\n\thugetlb_count_init(mm);\n\n\tif (current->mm) {\n\t\tmm->flags = mmf_init_flags(current->mm->flags);\n\t\tmm->def_flags = current->mm->def_flags & VM_INIT_DEF_MASK;\n\t} else {\n\t\tmm->flags = default_dump_filter;\n\t\tmm->def_flags = 0;\n\t}\n\n\tif (mm_alloc_pgd(mm))\n\t\tgoto fail_nopgd;\n\n\tif (mm_alloc_id(mm))\n\t\tgoto fail_noid;\n\n\tif (init_new_context(p, mm))\n\t\tgoto fail_nocontext;\n\n\tif (mm_alloc_cid(mm, p))\n\t\tgoto fail_cid;\n\n\tif (percpu_counter_init_many(mm->rss_stat, 0, GFP_KERNEL_ACCOUNT,\n\t\t\t\t     NR_MM_COUNTERS))\n\t\tgoto fail_pcpu;\n\n\tmm->user_ns = get_user_ns(user_ns);\n\tlru_gen_init_mm(mm);\n\treturn mm;\n\nfail_pcpu:\n\tmm_destroy_cid(mm);\nfail_cid:\n\tdestroy_context(mm);\nfail_nocontext:\n\tmm_free_id(mm);\nfail_noid:\n\tmm_free_pgd(mm);\nfail_nopgd:\n\tfree_mm(mm);\n\treturn NULL;\n}"
    }
]